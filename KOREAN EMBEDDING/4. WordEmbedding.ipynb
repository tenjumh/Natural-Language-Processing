{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 수준 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>4.1 NPLM</h2></b>\n",
    "1) 모델 기본 구조\n",
    "- 기존 언어 모델의 단점<br>\n",
    "    (1) 학습 데이터에 존재하지 않는 n-gram이 포함된 문장이 나타날 확률 값을 0으로 부여<br>\n",
    "    (2) 문장의 장기 의존성을 포착해내기 어려움, 즉 n-gram모델의 n을 5이상으로 길게 설정 불가. n이 커질수록 그 등장 확률이 0인 단어 시퀀스가 기하급수적으로 늘어남<br>\n",
    "    (3) 단어/문장 간 유사도 계산 불가<br>\n",
    "![그림4-1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC4-1.PNG?raw=True)\n",
    "- NPLM은 단어 시퀀스가 주어졌을 때 다음 단어가 무엇인지 맞추는 과정에서 학습\n",
    "- n-gram언어임\n",
    "![수식4-1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-1.PNG?raw=True)\n",
    "- 문장 내 t번째 단어(wt)에 대응하는 단어 벡터 xt를 만드는 과정. (수식4-2)\n",
    "- NPLM은 수식4-1을 최대화하려한다. 이는 조건부확률 P(천리|발,없는,말이), P(간다|없는,말이,천리)를 각각 높인다는 의미\n",
    "- 수식4-1에서 wt는 문장의 t번째 단어\n",
    "- NPLM 구조 말단의 출력은 |V|차원의 스코어 벡터 y(wt)에 소프트맥스 함수를 적용한 |V|차원의 확률 벡터\n",
    "<br>\n",
    "\n",
    "![수식4-2](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-2.PNG?raw=True)\n",
    "- 문장 내 t번째 단어(wt)에 대응하는 단어 벡터 xt를 만드는 과정. (수식4-2)\n",
    "- |V| X m 크기를 갖는 커다란 행렬 C에서 wt에 해당하는 벡터를 참조한 형태. |V|는 어휘 집합 크기, m은 xt 차원수, C행렬의 원소값은 초기에 랜덤 설정\n",
    "![그림4-2](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC4-2.PNG?raw=True)\n",
    "- 그림4-2에서 어휘 집합에 속한 단어가 5개(|V|)이고 wt가 이 가운데 4번째라고 가정하면\n",
    "- C(wt)는 행렬 C와 wt에 대항하는 원핫벡터를 내적한 것\n",
    "- 즉 C라는 행렬에서 wt에 해당하는 행만 참조하는 것과 동일\n",
    "![수식4-3](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-3.PNG?raw=True)\n",
    "- 없는, 말이, 천리 이렇게 세 개 단어가 주어졌을 때 \"간다\"라는 단어를 예측해야 하는 상황이라고 하자\n",
    "- 우선 세 개 각각의 단어의 인덱스 값을 확인\n",
    "- 수식4-3처럼 세 개 단어에 해당하는 열 벡터를 C에서 참조한 뒤, 이 세 개 벡터를 묶어주면 NPLM의 입력 벡터 x가 된다.\n",
    "- n은 고려 대상 n-gram 개수\n",
    "- \"발 없는 말이 천리 간다.\"라면 입력 단어들과 예측 대상 단어(간다)를 포함해 모두 네 개이기 때문에 n은 4이며 예측 대상 단어가 문장에서 다섯 번째로 등장하기 때문에 t는 5\n",
    "<br>\n",
    "- NPLM 스코어 벡터 y 계산\n",
    "![수식4-4](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-4.PNG?raw=True)\n",
    "- y(wt)에 소프트맥스 함수를 적용한 뒤 이를 정답 단어인 \"간다\"의 인덱스와 비교해 역전파하는 방식으로 학습\n",
    "- 수식4-5는 NPLM의 학습 파라미터 차원 수를 정리\n",
    "- NPLM는 파라미터의 종류가 많고, 그 크기가 큰 편 --> 최신 임베딩은 파라미터를 줄이고 품질은 높이는 쪽으로 발전\n",
    "![수식4-5](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-5.PNG?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) NPLM과 의미 정보\n",
    "- NPLM은 단어의 의미를 어떻게 임베딩 하나?\n",
    "![그림4-3](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC4-3.PNG?raw=True)\n",
    "- 우선 4-gram으로 생각해보고 타겟 단어가 walking인 문장은 그림4-4와 같다.\n",
    "![그림4-4](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC4-4.PNG?raw=True)\n",
    "- 따라서 The, A, cat, dog, is, was는 walking이라는 단어와 모종의 관계가 형성\n",
    "- The, A, cat, dog, is, was에 해당하는 C 행렬의 행 벡터들은 walking을 맞추는 과정에서 발생한 학습 손실을 최소화하는 그래디언트를 받아 동일하게 업데이트, 즉, The, A, cat, dog, is, was 벡터가 벡터 공간에서 같은 방향으로 조금씩 움직인다는 뜻\n",
    "- 네 번째 단어가 running인 데이터를 뽑아보면 그림4-5와 같다.\n",
    "![그림4-5](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC4-5.PNG?raw=True)\n",
    "- The, A, cat, dog, is, was 또한 running이라는 단어와 관계를 지니며 결과적으로 The, A, cat, dog, is, was벡터가 벡터 공간에서 같은 방향으로 업데이트됨\n",
    "- 다섯 번째 단어가 in인 데이터를 뽑으면 그림4-6이됨\n",
    "![그림4-6](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC4-6.PNG?raw=True)\n",
    "- <b>학습데이터에 없는 \"The mouse is running in a room\"이라는 문장의 등장 확률을 예측한다면\n",
    "    1) n-gram기반 모델은 학습 데이터에서 한 번도 등장하지 않은 패턴에 대해서는 등장 확률을 \"0\"으로 부여하지만\n",
    "    2) NPLM에서는 학습하지 않았더라도 문맥이 비슷한 다른 문장을 참고해 확률을 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>4.2 Word2Vec</h2></b>\n",
    "- 종류 : Skip-Gram과 CBOW 모델과 두 모델의 근간인 네거티브 샘플링의 학습 최적화 기법\n",
    "<b>1) 모델 기본 구조</b>\n",
    "![그림4-7](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC4-7.PNG?raw=True)\n",
    "- CBOW : 주변에 있는 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습\n",
    "- Skip-Gram : 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정에서 학습\n",
    "- 그림4-7을 보면 <br>\n",
    "    CBOW - 입력, 출력 학습 데이터 쌍이 {문맥 단어 4개, 타깃 단어 하나}인 반면<br>\n",
    "    Skip-gram - 학습 데이터는 {타깃 단어, 타깃 직전 두번째 단어}, {타깃 단어, 타깃 직전 단어}, {타깃 단어, 타깃 다음 단어}, {타깃 단어, 타깃 다음 두 번째 단어} 4개 쌍이 됨<br>\n",
    "<br>\n",
    "<b>2) 학습 데이터 구축</b>\n",
    "- Skip-gram 모델의 학습 데이터 구축 과정\n",
    "- 그림4-8의 문장에서<br>\n",
    "![그림4-8](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC4-8.PNG?raw=True)<br>\n",
    "- 학습 데이터 구축하는 과정은 그림4-9와 같다.\n",
    "- 포지티브 샘플 : 타깃 단어(t)와 그 주변에 실제로 등장한 문맥 단어(c)쌍을 가리킴\n",
    "- 네거티브 샘플 : 타깃 단어(t)와 그 주변에 등장하지 않은 단어(랜덤 추출) 쌍\n",
    "- 그림4-9에서 윈도우는 2로 설정(포지티브 샘플을 만들 때 타깃 단어 앞뒤 두 개씩만 고려)<br>\n",
    "![그림4-9](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC4-9.PNG?raw=True)<br>\n",
    "\n",
    "- 전체 말뭉치를 단어별로 슬라이딩해 가면서 학습 데이터를 만듬\n",
    "- 예를 들어 \"를\"이라는 단어가 타깃 단어(t)가 되고 \"빨래\"는 문맥 단어(c)가 됨\n",
    "- Skip-gram은 어휘집합에 속하는 단어 수를 모두 계산해야해 비효율적\n",
    "- 이것의 해결안으로 \"네거티브 샘플링\" : 타깃 단어와 문맥 단어 쌍이 주어졌을 때 해당 쌍이 포지티브 샘플(+)인지, 네거티브 샘플(-)인지 이진 분류하는 과정에서 학습\n",
    "- 네거티브 샘플링 방식 : 1개의 포지티브 샘플과 k개의 네거티브 샘플만 계산(기존에는 1 step에서 전체 단어를 모두 계산해야 했음)\n",
    "- 그림4-9는 k를 2로 설정한 예시, 1개의 포지티브 샘플과 2개의 네거티브 샘플 즉 3개만 학습<br>\n",
    "![수식4-6](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-6.PNG?raw=True)<br>\n",
    "\n",
    "- 네거티브 샘플은 말뭉치에 자주 등장하지 않는 희귀한 단어가 더 잘 뽑힐 수 있도록 설계\n",
    "-수식4-6dptj <b>f(wi)란 해당 단어가 말뭉치에 차지하는 비률(해당단어빈도/어휘집합크기)를 의미</b>\n",
    "- 예를 들어 용과 미르 단어의 구성 비율이 각각 0.99, 0.01일 때\n",
    "- 각각 네거티브로 뽑힐 확률은 수식4-7과 같다.<br>\n",
    "![수식4-7](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-7.PNG?raw=True)<br>\n",
    "\n",
    "- 서브샘플링 : 자주 등장하는 단어는 학습에서 제외하는 기법\n",
    "- 그림4-9에서 볼수 있듯이 Skip-gram모델은 말뭉치로 많은 학습 데이터 쌍을 만들기 때문에 비효율적이여서 수식4-8과 같이 서브샘플링 확률을 적용<br>\n",
    "![수식4-8](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-8.PNG?raw=True)<br>\n",
    "- 만일 f(wi)가 0.01로 나타나는 빈도 높은 단어(예컨대 조사 은/는)는 P(wi)가 0.9684나 돼서 해당 단어가 가질 수 있는 100번의 학습 기회에서 3-4번 정도는 학습에서 제외.\n",
    "- 반대로 등장 비율이 적어 P(wi)가 0에 가깝다면 해당 단어가 나올 때마다 학습."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3) 모델 학습</b>\n",
    "- Skip-gram 모델은 타깃 단어와 주변 단어 쌍이 포지티브 샘플(+)인지 아닌지를 예측하는 과정에서 학습\n",
    "- 수식4-9의 조건부확률 최대화, 즉 포지티브 샘플 단어 쌍을 입력받았을 때 포지티브 샘플로 예측해야 함\n",
    "![수식4-9](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-9.PNG?raw=True)\n",
    "\n",
    "- Skip-gram모델의 학습 파라미터는 U와 V 행렬 두 개\n",
    "- 둘의 크기는 어휘 집합 크기(|V|) X 임베딩 차원 수(d)로 동일\n",
    "- U와 V는 각각 타깃 단어와 문맥 단어에 대응\n",
    "- ut는 타깃 단어(t) \"빨래\"에 해당하는 U의 행 벡터\n",
    "- vc는 문맥 단어(c) \"속옷\"에 해당하는 V의 열 벡터\n",
    "![그림4-10](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC4-10.PNG?raw=True)\n",
    "\n",
    "- 포지티브 수식을 최대화하려면 분자 up, 분모 down\n",
    "- 분모 down >>> exp(-ut*vc)를 최소화\n",
    "- 즉 ut*vc의 내적을 최대화 >>> 두 벡터의 내적은 코사인 유사도와 비례\n",
    "- 따라서 내적 값의 상향은 포지티브 샘플 t와 c에 해당하는 단어 벡터 간 유사도 UP\n",
    "\n",
    "![수식4-10](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-10.PNG?raw=True)\n",
    "\n",
    "- 네거티브 수식을 최대화하려면 분자 up\n",
    "- exp(-ut*vc)를 최대화\n",
    "- 즉 ut*vc의 내적을 최소화 >>> 두 벡터의 내적은 코사인 유사도와 비례\n",
    "- 따라서 내적 값의 상향은 포지티브 샘플 t와 c에 해당하는 단어 벡터 간 유사도 DOWN\n",
    "\n",
    "- <b>상기 두식을 포함하여 Skip-gram모델이 최대화해야 하는 로그우드 함수 - 수식4-11\n",
    "    \n",
    "![수식4-11](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-11.PNG?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec skip-gram 모델로 위키백과, 네이버 등 말뭉치 합친것 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/notebooks/embedding/data/tokenized/corpus_mecab.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9caba2ced348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_fname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_fname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/notebooks/embedding/data/tokenized/corpus_mecab.txt'"
     ]
    }
   ],
   "source": [
    "# shell에서 : docker run -it --rm ratsgo/embedding-cpu bash\n",
    "# docker에서 : git pull origin master\n",
    "# docker에서 : bash preprocess.sh dump-tokenized\n",
    "# docker에서 : cd /notebooks/embedding\n",
    "# docker에서 : cat wiki_ko_mecab.txt ratings_mecab.txt korquad_mecab.txt > corpus_mecab.txt\n",
    "\n",
    "corpus_fname = \"/notebooks/embedding/data/tokenized/corpus_mecab.txt\"\n",
    "model_fname = \"/notebooks/embedding/data/word-embeddings/word2vec/word2vec\"\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "corpus = [sent.replace('/n', '').strip() for sent in open(corpus_fname, 'r', encoding='UTF-8').readlines()]\n",
    "model = Word2Vec(corpus, size=100, workers=4, sg=1)\n",
    "model.save(model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec skip-gram 모델로 학습 완료된 데이터에서 코사인 유사도 상위 단어 목록 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4cb6fa1247d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_eval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordEmbeddingEvaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordEmbeddingEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/fxk/PycharmProjects/tenjumh/Study/KOREAN EMBEDDING/data/word-embeddings/word2vec/word2vec\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"word2vce\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mecab\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"희망\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from models.word_eval import WordEmbeddingEvaluator\n",
    "model = WordEmbeddingEvaluator(\"C:/Users/fxk/PycharmProjects/tenjumh/Study/KOREAN EMBEDDING/data/word-embeddings/word2vec/word2vec\", method=\"word2vce\", dim=100, tokenizer_name=\"mecab\")\n",
    "model.most_similar(\"희망\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>4.8 가중임베딩</h2></b>\n",
    "- 단어 임베딩을 문장 수준 임베딩으로 확장<br>\n",
    "<b>8.1 모델 개요</b><br>\n",
    "- 문서 내 단어의 등장은 저자가 생각한 주제에 의존한다 가정\n",
    "- 주제에 따라 단어의 사용 양상이 달라짐 => 주제 벡터(discourse vector) 개념 도입\n",
    "- <b>주제 벡터 c(s)</b>가 주어졌을 때 <b>어떤 단어 w</b>가 나타날 확률을 수식 4-25와 같이 정의<br>\n",
    "![수식4-25](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-25.PNG?raw=True)\n",
    "\n",
    "- 수식4-25에 ~c(s)는 c(s)로부터 어떤 절차를 거쳐 도출하는 벡터이나 주제 벡터 c(s)와 비슷한 역할을 하는 임의 벡터라 정의\n",
    "- Z는 수식4-25 우변 두 번째 항이 확률 값이 되도록 해주는 노멀라이즈 팩터\n",
    "- 우변의 첫 번째 항은 단어 w가 주제와 상관없이 등장할 확률<br>\n",
    "    예 : 조사 \"을/를\", \"이/가\"의 경우 P(w)가 높은 축에 속함, 자주 등장하니까<br>\n",
    "- 우변의 두 번째 항은 단어 w가 주제와 관련을 가질 확률<br>\n",
    "    주제벡터 c(s)와 w에 해당하는 단어벡터 v(w)가 유사할수록(내적값이 클수록) 그 값이 커진다.\n",
    "    예 : 주제가 \"정치\"일 때, \"사퇴\", \"경제\"일 때, \"인수\"\n",
    "- a(알파)는 하이퍼파라미터<br>\n",
    "<br>\n",
    "![수식4-26](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-26.PNG?raw=True)\n",
    "- 단어시퀀스 = 문장\n",
    "- 문장 등장 확률(=단어들이 동시에 등장할 확률)은 문장에 속한 모든 단어의 등장 확률의 누적의 곱\n",
    "- 그런데 확률을 누적해서 곱하는 것은 무한이 작아지기에 로그를 취해 덧셈으로 대체<br>\n",
    "** 어떠한 값들의 누적곱을 log를 취하면 덧셈으로 대체 가능<br>\n",
    "** 로그란 100 → 0, 101→ 1, 102 → 2, 103 → 3……의 방식으로 수를 대응시키는 것이다.<br>\n",
    "이것을 log라는 기호를 이용해 나타내면 <br>\n",
    "log 100=0 , log 10 1=1 , log102=2 , log103=3 ……이 된다.<br>\n",
    "즉 수를 10n의 형태로 나타냈을 때, 지수 n이  log10n인 것이다.<br>\n",
    "- 누적곱 --> 로그 무한 덧셈 -> (테일러급수) 함수로 표현\n",
    "![수식4-27](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-27.PNG?raw=True)\n",
    "- 테일러 급수를 쓰면 무한합의 n번째 항까지만 써서 원래함수를 근사\n",
    "- 단어 w가 등장할 확률을 최대화하는 주제벡터 c(s)/~c(s)를 찾아야 함\n",
    "- w가 등장할 확률을 최대화하는 c(s)를 찾으면 이 때 c(s)/~c(s)는 해당 단어(w)를 제일 잘 설명하는 주제 벡터\n",
    "![수식4-28](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-28-29.PNG?raw=True)\n",
    "- (1-a)/aZ를 a로 치환한 후 argmax를 취해 정리한 결과 [수식4-28]\n",
    "- max(C+c・g) = g/||g||이 성립한다고 한다.???\n",
    "- 문장의 등장 확률을 최대한으로 높이는 주제 벡터를 수식4-29와 같이 정리\n",
    "- 우리가 관찰하고 있는 문장이 등장할 확률을 최대화하는 주제 벡터 c(s)/~c(s)는 문장에 속한 단어들에 해당하는 단어 벡터에 가중치를 곱해 만든 새로운 벡터들의 합에 비례\n",
    "- 새로운 단어 벡터를 만들 때의 가중치는 해당 단어가 말뭉치에 얼마나 자주 등장하는지, 즉 P(w)를 감안한다.\n",
    "- <b>희귀한 단어(P(w)가 작다)라면 높은 가중치를 곱해 해당 단어의 벡터의 크기를 키우고 고빈도 단어라면 해당 벡터의 크기를 줄인다.</b>\n",
    "- 정보성이 높은, 희귀한 단어에 가중치를 높게 주는 TF-IDF의 철학과\n",
    "- 문장 내 단어의 등장 순서를 고려하지 않는다는 bag of words의 철학과 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>8.2 모델 구현</b><br>\n",
    "- 문장을 토큰으로 나눈 뒤 해당 토큰들에 대응하는 벡터들의 합으로 문장의 임베딩 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![수식4-11](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D4-11.PNG?raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
