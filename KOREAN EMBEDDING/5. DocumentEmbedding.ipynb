{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 수준 임베딩\n",
    "- 잠재 의미 분석(LSA) : 행렬분해\n",
    "- 잠재 디리클레 할당(LDA) : 확률모델, Doc2Vecm ELMo, GPT, BERT\n",
    "- 특히, GPT, BERT등은 Self Attention기반의 트랜스포머 네트워크로 구성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>1. 잠재 의미 분석</h3></b><br> \n",
    "- 잠재 의미 분석 : 단어-문서 행렬이나 TF-IDF 행렬, 단어-문맥 행렬 또는 PMI행렬에 특이값 분해로 차원 축소를 시행하고 단어에 해당하는 벡터를 취해 임베딩\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>3. 잠재 디리클레 할당(LDA)</h3></b><br> \n",
    "\n",
    "![LDA도식](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/5%EC%9E%A5/lda%EB%8F%84%EC%8B%9D.png?raw=True)\n",
    "- LDA는 토픽별 단어의 분포, 문서별 토픽의 분포를 추정\n",
    "- 특정 토픽에 특정 단어가 나타날 확률, 위 도식에서 노란색 토픽엔 gene이라는 단어가 등장확률이 0.04, dna는 0.02, genetic은 0.01로 노란색 토픽은 대략 \"유전자\"라는 주제 예상\n",
    "- 주어진 문서를 보면 노란색 토픽에 해당하는 단어가 많다. 따라서 문서의 메인 주제는 노란색 토픽일 가능성이 높다.\n",
    "- LDA는 글쓰기 예를 자주 든다.\n",
    "- 글을 쓰기 전에 (1) 주제를 정한다. (2) 실제 글을 쓸 때 어떤 단어를 쓸지 결정한다.\n",
    "- LDA는 (1) 말뭉치로부터 얻은 토픽 분포로부터 토픽을 뽑는다. (2) 해당 토픽에 해당하는 단어를 뽑는다.\n",
    "- LDA는 말뭉치 이면에 존재하는 정보를 추론해기에 잠재라는 이름을 붙인것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LDA도식](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/5%EC%9E%A5/LDA1.jpg?raw=True)\n",
    "![LDA도식](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/5%EC%9E%A5/LDA2.jpg?raw=True)\n",
    "![LDA도식](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/5%EC%9E%A5/LDA3.jpg?raw=True)\n",
    "![LDA도식](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/5%EC%9E%A5/LDA4.jpg?raw=True)\n",
    "![LDA도식](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/5%EC%9E%A5/LDA5.jpg?raw=True)\n",
    "![LDA도식](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/5%EC%9E%A5/LDA6.jpg?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling, LDA 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4be4e93795ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Counter로 구성된 리스트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# 각 Counter는 각 문서를 의미\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdocument_topic_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# document_topic_counts[3][1] : 3번째 문서에서 토픽1과 관련있는 단어 수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "### 변수 선언\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# \" n(d,k)\"\n",
    "# 각 토픽이 각 문서에 할당되는 횟수\n",
    "# Counter로 구성된 리스트\n",
    "# 각 Counter는 각 문서를 의미\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "# document_topic_counts[3][1] : 3번째 문서에서 토픽1과 관련있는 단어 수\n",
    "\n",
    "# \" V(k,w)\"\n",
    "# 각 단어가 각 토픽에 할당되는 횟수\n",
    "# Counter로 구성된 리스트\n",
    "# 각 Counter는 각 토픽을 의미\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "# topic_word_counts[2]['nlp'] : 토픽2에 nlp단어가 연관지어 등장한 횟수\n",
    "\n",
    "# \" 시그마 V(k,j)\"\n",
    "# 각 토픽에 할당되는 총 단어수\n",
    "# 숫자로 구성된 리스트\n",
    "# 각각의 숫자는 각 토픽을 의미함\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# \" 시그마 n(d,i)\"\n",
    "# 각 문서에 포함되는 총 단어수\n",
    "# 숫자로 구성된 리스트\n",
    "# 각각의 숫자는 각 문서를 의미함\n",
    "document_lengths = map(len, documents)  # map(a, b) b의 변수를 a적용하여 리스트\n",
    "\n",
    "# 단어 종류의 수\n",
    "distinct_word = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "\n",
    "# 총 문서의 수\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d번째 문서 i번째 단어의 토픽 z(d,i)가 j번째 할당될 확률 = A X B \n",
    "# A : p_topic_given_document\n",
    "# B : p_word_given_topic\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    # 문서 d의 모든 단어 가운데 topic에 속하는\n",
    "    # 단어의 비율 (alpha를 더해 smoothing)\n",
    "    return ((document_topic_counts[d][topic] + alpha) / (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    # topic에 속한 단어 가운데 word의 비율\n",
    "    # (beta를 더해 smoothing)    \n",
    "    return ((topic_word_counts[topic][word] + beta) / (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word ,k):\n",
    "    # 문서와 문서의 단어가 주어지면\n",
    "    # k번째 토픽의 weight를 반환\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A X B 완료\n",
    "# 깁스 샘플링을 통해 z(d,i)에 새로운 topic 할당\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "import random\n",
    "def sample_from(weights):\n",
    "    # i를 weights[i] / sum(weights)\n",
    "    # 확률로 반환\n",
    "    total = sum(weights)\n",
    "    # 0과 total 사이를 균일하게 선택\n",
    "    rnd = total * random.random()\n",
    "    # 아래 식을 만족하는 가장 작은 i를 반환\n",
    "    # weights[0] + ... + weights[i] >= rnd \n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [[\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference에 필요한 기초 데이터 만듬\n",
    "# 토픽수 K 등 하이퍼파라메터를 정하고, 각 단어를 임의의 토픽에 배정한 뒤 필요한 숫자 카운팅\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# topic 수 지정\n",
    "K=4\n",
    "\n",
    "# 각 단어를 임의의 토픽에 랜덤 배정\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "\n",
    "# 위와 같이 랜덤 초기화한 상태에서 \n",
    "# AB를 구하는 데 필요한 숫자를 세어봄\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TF-IDF행렬](https://github.com/tenjumh/Natural-Language-Processing/blob/master/KOREAN%20EMBEDDING/image/5%EC%9E%A5/LDA-3\n",
    ".jpg?raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
