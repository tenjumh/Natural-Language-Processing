{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 벡터가 어떻게 의미를 가지게 되는가\n",
    "\n",
    "# 자연어 계산과 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 = 자연어를 컴퓨터가 처리할 수 있는 숫자들의 나열인 벡터로 변경<br> \n",
    "- 임베딩할 때 쓰이는 통계 정보<br> \n",
    "1) 문장에 어떤 단어가 많이 쓰였는지<br> \n",
    "2) 단어가 어떤 순서로 등장하는지<br> \n",
    "3) 문장에 어떤 단어가 같이 나타났는지<br> \n",
    "\n",
    "![임베딩을 만드는 세가지 철학](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9D%84%20%EB%A7%8C%EB%93%9C%EB%8A%94%20%EC%84%B8%EA%B0%80%EC%A7%80%20%EC%B2%A0%ED%95%99.PNG?raw=True)\n",
    "\n",
    "- 백오브워즈(Bag of Words) : <br> \n",
    "1) 어떤 단어가 많이 쓰였는지 정보 중시<br> \n",
    "2) 저자의 의도는 단어 사용 여부나 빈도에 드러난다고 가정<br> \n",
    "3) 단어의 순서 정보는 무시<br> \n",
    "4) Term Frequency-Inverse Document Frequency<br> <br> \n",
    "- 언어모델(language model)<br> \n",
    "1) 단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률 부여<br>\n",
    "2) ELMo, GPT 등 뉴럴 네트워크 기반의 언어 모델<br> <br> \n",
    "- 분포가정(Distributional hypothesis)<br> \n",
    "1) 문장에서 어떤 단어가 같이 쓰였는지를 중요<br> \n",
    "2) 단어의 의미는 그 주변 문맥을 통해 유추\n",
    "3) 점별상호 정보량(Pointwise Mutual Information)\n",
    "4) Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 세 철학은 서로 연관\n",
    "1) 어떤 단어가 문장에서 주로 나타나는 순서는 해당 단어의 주변 문맥과 관계를 가지며 어떤 단어 쌍이 얼마나 같이 자주 나타나는지와 관련한 정보를 수치화하기 위해 개별 단어와 단어 쌍의 빈도 정보를 적극 활용\n",
    "2) 세 철할은 말뭉치의 통계적 패턴을 서로 다른 각도에서 분석하는 것이며 상호보완적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어떤 단어가 많이 쓰였는가\n",
    "\n",
    "<b>1. 백오브워즈 가정</b><br>\n",
    "- 문장을 단어들로 나누고 이들을 중복집합에 넣어 임베딩으로 활용\n",
    "- 그럼 2-1처럼 단어들을 Bag에 넣고 흔들어 그 빈도를 세어 놓은 것\n",
    "![백오브워즈 임베딩](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/bag%20of%20words%20embedding.PNG?raw=True)\n",
    "\n",
    "-비슷한 문서라면 단어 빈도 또는 단어 등장 여부 역시 비슷할 것이고 임베딩 역시 유사할 것\n",
    "-\"사랑 손님과 어머니\"와 \"삼포 가는 길\" 관련성은 상대적으로 높다.\n",
    "![단어-문서행렬](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EB%8B%A8%EC%96%B4%EB%AC%B8%EC%84%9C%ED%96%89%EB%A0%AC.PNG?raw=True)\n",
    "\n",
    "-정보 검색분야에서는 사용자의 질의에 가장 적절한 문서를 보여줄 때 질의를 백오브워즈 임베딩으로 변환하고 \"질의\"와 \"검색 대상 문서\" 임베딩 간 코사인 유사도가 높은 것을 노출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. TF-IDF</b><br>\n",
    "- 단어 빈도 또는 등장 여부를 그대로 임베딩 쓰면 어떤 문서에든 쓰여서 해당 단어가 많이 나타났다하더라도 문서의 주제를 가늠하기 어려운 경우가 있다.<br>\n",
    "- 이 단점을 보완하기 위해 TF-IDF기법있다.<br>\n",
    "- TF-IDF 역시 단어 등장 순서를 고려하지 않아 백오브워즈 임베딩임.<br>\n",
    "![TF-IDF](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/TF-IDF.PNG?raw=True)\n",
    "1) TF - 어떤 단어가 특정문서에 얼마나 많이 쓰였는지 빈도<br>\n",
    " 예)문서1:A단어 10번, 문서3:A단어 5번 --> 문서1-단어A TF = 10, 문서3-단어A TF = 5<br>\n",
    "2) DF - 특정 단어가 나타난 문서의 수<br>\n",
    " 예)A라는 단어가 말뭉치 전체에서 문서1, 문서3에만 등장했다면 DF = 2<br>\n",
    " - DF가 클수록 다수문서에 쓰이는 범용적인 단어<br>\n",
    "3) TF는 같은 단어라도 문서마다 다른 값, DF는 문서가 달라지더라도 단어가 같다면 동일<br>\n",
    "4) IDF - 전체 문서 수(N)를 해당 단어의 DF로 나눈 뒤 로그를 취한 값<br>\n",
    " -값이 클수록 특이한 단어임, 즉 단어의 주제 예측 능력과 직결<br>\n",
    "\n",
    "- 결론, TF-IDF가 지향하는 원리는<br>\n",
    " 1) 어떤 단어의 주제 예측 능력이 강할 수록 가중치가 커지고 반대의 경우 작어진다.<br>\n",
    " ![TF-IDF행렬](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/TF-IDF%ED%96%89%EB%A0%AC%EC%98%88.PNG?raw=True)\n",
    " 2) 표 2-3을 보면 '담배'라는 단어는 조사 '를'보다 TF-IDF값이 크다<br>\n",
    "    '를'을 봐서는 주제 추측 곤란, 담배가 나왔다면 추측 용이<br>\n",
    " 3) 표 2-3처럼 정보성이 없는 단어들은 가중치가 0에 가까워짐<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Deep Averaging Network</b><br>\n",
    "- 백오브워즈 가정의 뉴럴 네트워크 버전, 모델 콘셉트 하기 그림 참고\n",
    "\n",
    "![DAN](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/DeepAveragingNetwork.PNG?raw=True)\n",
    "- 문장 내에 어떤 단어가 쓰였는지, 쓰였다면 얼마나 많이 쓰였는지 그 빈도만 따짐\n",
    "- 순서는 상관없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어가 어떤 순서로 쓰였는가\n",
    "\n",
    "<b>1. 통계 기반 언어 모델</b><br>\n",
    "- 단어 스퀀스에 확률을 부여하는 모델<br>\n",
    "- 단어 n개 주어진 상황이라면 언어 모델은 n개 단어가 동시에 나타날 확률, 즉 P(w1, w2,..wn)을 반환<br>\n",
    "- 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습<br>\n",
    "- 어떤 문장이 그럴듯한지, 주어진 단어 시퀀스 다음 단어는 무엇이 오는지 알게 됨<br>\n",
    "- 그림 2-3 자연스러운 한국어 문장에 높은 확률 값을 부여<br>\n",
    "![한국어언어모델](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%ED%95%9C%EA%B5%AD%EC%96%B4%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8%20%EC%98%88%EC%94%A8.PNG?raw=True)\n",
    "- n-gram이란 n개 단어를 뜻하는 용어(말뭉치 내 단어들을 n개씩 묶어서 그 빈도를 학습)<br>\n",
    " 1) \"난폭, 운전\", \"눈, 뜨다\" 등은 2-gram = bigram(바이그램)<br>\n",
    " 2) \"누명, 을, 쓰다\" 3-gram = trigram<br>\n",
    " 3) \"바람, 잘, 날, 없다\" 4-gram<br>\n",
    "- 표 2-4는 네이버 영화 리뷰 말뭉치이며 표현이 등장한 횟수를 표시(띄어쓰기 단위인 어절을 하나의 단위로)\n",
    "- 표 2-4에 \"내 마음 속에 영원히 기억될 최고의 명작이다\"는 한번도 등장하지 않음.\n",
    "- 따라서 말뭉치로 학습한 언어 모델은 해당 표현이 나올 확률 <b>\"0\"</b>\n",
    "![네이버](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EB%84%A4%EC%9D%B4%EB%B2%84%EC%98%81%ED%99%94%20%EB%A7%90%EB%AD%89%EC%B9%98%20%EB%93%B1%EC%9E%A5%ED%9A%8C%EC%88%98.PNG?raw=True)\n",
    "\n",
    " 1. \"내 마음 속에 영원히 기억될 최고의\"라는 표현 다음 \"명작이다\"가 나타날 확률은 조건부확률(Conditional Probability)의 정의를 활용해 <b>최대우도추정법</b>은 수식 2-2과 같다<br>\n",
    "<b>우변의 분자가 \"0\"(빈도)이여서 전체 값이 \"0\"이 된다.</b> <br>\n",
    "![수식2-2](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EB%AA%85%EC%9E%91%EC%9D%B4%EB%8B%A4%EA%B0%80%20%EB%82%98%ED%83%80%EB%82%A0%20%ED%99%95%EB%A5%A0.PNG?raw=True)\n",
    "    \n",
    " 2. n-gram모델을 쓰면 일부 해결<br>\n",
    "     - n-1개 단어의 등장 확률로 전체 단어 시퀀스 등장 확률을 근사\n",
    "     - 한 상태의 확률은 그 직전 상태에만 의존 : 마코프 가정\n",
    "     -\"내 마음 속에 영원히 기억될 최고의\" 다음에 \"명작이다\"가 나타날 확률을 bygram 모델로 근사하면 수식 2-3과 같다.\n",
    "     -\"최고의 명작이다\" 빈도를 \"최고의\" 빈도로 나눠준 값<br>\n",
    "      즉, \"명작이다\" 직전의 1개 단어만 보고 전체 단어 시퀀스 등장 확률을 근사\n",
    "![수식2-3](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EB%B0%94%EC%9D%B4%EA%B7%B8%EB%9E%A8%20%EA%B7%BC%EC%82%AC%20%EC%98%88%EC%8B%9C.PNG?raw=True)<br>\n",
    "\n",
    " 3. bygram 모델에서 \"내 마음 속에 영원히 기억될 최고의 명작이다\"라는 단어 시퀀스가 나타날 확률은 얼마나 될까? <수식 2-4><br>\n",
    "  (1) \"내\"가 단독으로 등장활 확률 계산<br>\n",
    "  (2) \"내\" 다음에 \"마음\"이 나타날 확률 곱함<br>\n",
    "  (3) \"마음\" 다음에 \"속에\"가 등장할 확률 곱함<br>\n",
    "  (4) 계속 슬라이딩해가면서 계산하면 수식 2-4가 됨 (|V|는 어휘 집합에 속한 단어 수)<br>\n",
    "![수식2-4](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EB%B0%94%EC%9D%B4%EA%B7%B8%EB%9E%A8%20%EA%B7%BC%EC%82%AC%20%EC%98%88%EC%8B%9C2.PNG?raw=True)\n",
    "\n",
    " 4. 바이그램(bygram)모델을 일반화한 식은 수식 2-5와 같다.<br>\n",
    "     - n-gram모델은 바이그램 모델의 확장판으로 직전 1개만 참고하는 바이그램 모델과 달리 전체 단어 시퀀스 등장 확률 계산 시 직전 n-1개 단어의 히스토리를 본다.<br>\n",
    "     ![수식2-5](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EB%B0%94%EC%9D%B4%EA%B7%B8%EB%9E%A8%20%EB%AA%A8%EB%8D%B8.PNG?raw=True)\n",
    "  \n",
    " 5. 데이터에 한 번도 등장하지 않는 n-gram\n",
    "     - 그림 2-4에서 학습 데이터에 \"아이는\" 다음에 \"또바기\"라는 단어가 한 번도 등장하지 않았다면 예측 단계에서 \"그 아이는 또바기 인사를 잘한다\"라는 문장이 등장할 확률이 \"0\"이 된다.<br>\n",
    "     ![그림2-4](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/ngram%EB%AA%A8%EB%8D%B8%20%ED%95%9C%EA%B3%84.PNG?raw=True)\n",
    "\n",
    " 6. 이를 위해 백오프(back-off), 스무딩(smoothing) 등의 방식이 제안<br><br>\n",
    "     1) 백오프란 : n-gram 등장 빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사\n",
    "         - n을 크게 하면 할수록 등장하지 않는 케이스가 많아질 가능성이 높다.\n",
    "         - 7-gram모델을 적용하면 네이버 리뷰 말뭉치에서 \"내 마음 속에 영원히 기억될 최고의 명작이다\"의 등장 빈도는 0이 된다.\n",
    "         - 이를 백오프 방식(N을 사로 줄임)으로 7-gram빈도를 근사하면 수식 2-6과 같다.\n",
    "         - a, b는 실제 빈도와의 차이를 보정해주는 파라미터\n",
    "         - 물론 빈도가 1 이상인 7-gram에 대해서는 백오프하지 않고 해당 빈도를 그대로 사용\n",
    "![수식2-6](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EB%B0%B1%EC%98%A4%ED%94%84%EA%B8%B0%EB%B2%95%EC%98%88%EC%8B%9C.PNG?raw=True)\n",
    "     2) 스무딩 : 표 2-4와 같은 등장 빈도표에 모두 k만큼 더한 기법\n",
    "         - 이렇게 되면 \"내 마음 속에 영원히 기억될 최고의 명작이다\"의 빈도는 k(0+k)가 된다.\n",
    "         - 만약 k=1로 설정하면 이를 \"라플라스 스무딩\"\n",
    "         - 스무딩을 하면 높은 빈도를 가진 문자열 등장 확률을 일부 깍고 등장하지 않는 케이스에 작게 확률을 부여\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. 통계 기반 언어 모델</b><br>\n",
    "1. 뉴럴 네트워크 기반 언어 모델 컨셉트 : 그림 2-5\n",
    "![뉴럴](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EB%89%B4%EB%9F%B4%20%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%20%EA%B8%B0%EB%B0%98%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8%20%ED%99%9C%EC%9A%A9%20%EC%9E%84%EB%B2%A0%EB%94%A9.PNG?raw=True)\n",
    "    - 주어진 단어 시퀀스를 가지고 다음 단어를 맞추는 과정\n",
    "    - 학습이 완료되면 이들 모델의 중간 혹은 말단 계산 결과물을 단어나 문장의 임베딩으로 활용\n",
    "    - ELMo, GPT등 모델\n",
    "\n",
    "\n",
    "2. 마스크 언어 모델 : 큰 틀은 뉴럴과 유사하나 디테일에서 차이 : 그림 2-6\n",
    "![마스크](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EB%A7%88%EC%8A%A4%ED%81%AC%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8%20%EA%B8%B0%EB%B0%98%20%EC%9E%84%EB%B2%A0%EB%94%A9%20%EA%B8%B0%EB%B2%95.PNG?raw=True)\n",
    "    - 중간에 마스크를 씌워 놓고, 해당 마스크 위치에 어떤 단어고 올지 예측 과정 학습\n",
    "    - 문장 전체를 다 보고 중간에 있는 단어를 예측하기 때문에 양방향 학습 가능\n",
    "    - 기존 언어 모델 기법들 대비 임베딩 품질이 좋음\n",
    "    - BERT 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어떤 단어가 같이 쓰였는가\n",
    "<b>1. 분포 가정</b><br>\n",
    "- 자연어 처리에서 \"분포(distribution)\"란 윈도우 내에 동시에 등장하는 이웃 단어 또는 문맥의 집합<br>\n",
    "- 분포 가정<br>\n",
    "    1) 개별 단어의 분포는 그 단어가 문장 내에서 주로 어느 위치에 나타나는지?<br>\n",
    "    2) 이웃한 위치에 어떤 단어가 자주 나타나는지에 따라 달라짐.<br>\n",
    "    3) 어떤 단어 쌍이 비슷한 문맥 환경에서 자주 등장한다면 그 의미 또한 유사할 것<br>\n",
    "\n",
    "- 예를 들어 한국어의 \"빨래\", \"세탁\" 의미를 모른다고 하자<br>\n",
    "![그림2-7](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC2-7.PNG?raw=True)\n",
    "    1) \"빨래\", \"세탁\" - target word<br>\n",
    "    2) \"청소\", \"물\" - 주위에 등장한 문맥 단어<br>\n",
    "    3) 문맥에서 \"빨래\"는 \"청소\", \"요리\", \"물\", \"속옷\"과 같이 등장<br>\n",
    "    4) 문맥에서 \"세탁\"은 \"청소\", \"요리\", \"물\", \"옷\"과 같이 등장<br>\n",
    "    5) 분포 가정을 적용해 보면 \"빨래\", \"세탁\"은 유사할 가능성이 높다.<br>\n",
    "    6) 또한 문맥단어들끼리도 직간접적으로 관계를 지닐 가능성이 높다.<br>\n",
    "    7) 하지만 분포 정보와 의미 사이에는 논리적으로 직접적인 연관성은 없다. 따라서 둘 사이에 어떤 관계가 있는지 언어학적 관점에서 보자<br>\n",
    "    \n",
    "<b>2. 분포와 의미(1): 형태소</b><br>\n",
    "- 형태소란 의미를 가지는 최소 단위, \"철수가 밥을 먹는다\"에서 \"철수\"를 \"철\"과 \"수\"로 쪼개면 의미가 사라진다.\n",
    "- 언어학자들이 말하는 형태소란 계열 관계가 있다. 즉 해당 형태소 자리에 다른 형태소가 대치돼 쓰일 수 있따. \"철수가 밥을 먹는다\"에서 \"철수\"자리에 \"영희\"를 넣어도 \"밥\"대신 \"빵\"을 넣을 수 있고 이를 근거로 \"철수\"와 \"밥\"은 형태소이다.\n",
    "- 언어학자들의 형태소는 주변 문맥 정보를 바탕으로 형태소를 확인하는 방법이다.\n",
    "\n",
    "<b>3. 점별 상호 정보량(Pointwise Mutual Information)</b><br>\n",
    "- 두 확률변수 사이의 상관성을 계량화하는 단위\n",
    "- 두 확률변수가 완전 독립인 경우 그 값이 \"0\", 독립이라고 함은 단어 A가 나타나는 것이 단어 B의 등장할 확률에 전혀 영향을 주지 않고 반대도 마찬가지다.\n",
    "- PMI는 두 단어의 등장이 독립일 때 대비해 얼마나 자주 같이 등장하는지를 수치화\n",
    "![수식2-7](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D2-7.PNG?raw=True)\n",
    "- PMI는 분포가정에 따른 단어 가중치 할당 기법\n",
    "- 그림 2-9는 단어-문맥 행렬을 구축하는 과정을 개념적으로 나타낸 것\n",
    "- PMI행렬은 그림 2-9의 단어-문맥 행렬에 수식 2-7을 적용한 결과\n",
    "\n",
    "![그림2-9](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC2-9.PNG?raw=True)\n",
    "- 윈도우가 2라면 타깃 단어 앞뒤로 2개의 문맥 단어의 빈도를 계산\n",
    "- 타깃 단어가 \"빨래\"라면 \"에서\", \"속옷\", \"를\", \"하는\"이라는 문맥 단어가 계산의 대상이 되고 이들의 값을 1씩 올려줌.\n",
    "- 전체 빈도 수는 1000회, \"빨래\"가 등장한 횟수는 20회, \"속옷\"이 등장한 횟수는 15회, \"빨래\"와 \"속옷\"이 동시에 등장한 빈도는 10회라고 하면\n",
    "- \"빨래\"와 \"속옷\" 간 PMI값을 계산하면 다음과 같다.\n",
    "![수식2-8](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EC%88%98%EC%8B%9D2-8.PNG?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. Word2Vec</b><br>\n",
    "- 기본 구조는 그림 2-10\n",
    "- CBOW모델은 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습\n",
    "- Skip-gram 모델은 타깃 단어를 가지고 문맥 단어가 무엇일지 예측하는 과정에서 학습\n",
    "- 둘 모두 특정 타깃 단어 주변의 문맥(분포 정보)를 임베딩에 압축\n",
    "![그림 2-10](https://github.com/tenjumh/GraduateSchool/blob/master/Study/KOREAN%20EMBEDDING/image/%EA%B7%B8%EB%A6%BC2-10.PNG?raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
